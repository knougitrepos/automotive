{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MVP-V: 테스트 및 평가\n",
        "\n",
        "이 노트북은 KITTI 평가 메트릭을 사용하여 모델 성능을 평가합니다.\n",
        "\n",
        "## 목표\n",
        "- KITTI 평가 메트릭 (AP, mAP 등)\n",
        "- 성능 분석 및 리포트 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: 라이브러리 임포트\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "print(\"평가 모듈 준비 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: IoU 계산 함수\n",
        "def compute_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    두 바운딩 박스의 IoU 계산\n",
        "    box: [x1, y1, x2, y2]\n",
        "    \"\"\"\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "    \n",
        "    if x2 < x1 or y2 < y1:\n",
        "        return 0.0\n",
        "    \n",
        "    inter = (x2 - x1) * (y2 - y1)\n",
        "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    union = area1 + area2 - inter\n",
        "    \n",
        "    return inter / union if union > 0 else 0.0\n",
        "\n",
        "print(\"IoU 계산 함수 정의 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Precision-Recall 곡선 및 AP 계산\n",
        "def calculate_ap(recalls, precisions):\n",
        "    \"\"\"\n",
        "    Average Precision (AP) 계산\n",
        "    \"\"\"\n",
        "    # 11-point interpolation\n",
        "    ap = 0.0\n",
        "    for t in np.arange(0, 1.1, 0.1):\n",
        "        if np.sum(recalls >= t) == 0:\n",
        "            p = 0\n",
        "        else:\n",
        "            p = np.max(precisions[recalls >= t])\n",
        "        ap += p / 11.0\n",
        "    return ap\n",
        "\n",
        "def evaluate_detections(gt_boxes, pred_boxes, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    탐지 결과 평가 (클래스별 AP 계산)\n",
        "    \n",
        "    Args:\n",
        "        gt_boxes: list of [x1, y1, x2, y2, class_id]\n",
        "        pred_boxes: list of [x1, y1, x2, y2, class_id, confidence]\n",
        "        iou_threshold: IoU 임계값\n",
        "    \n",
        "    Returns:\n",
        "        results: 클래스별 AP 및 전체 mAP\n",
        "    \"\"\"\n",
        "    # 클래스별로 분리\n",
        "    classes = set([box[4] for box in gt_boxes] + [box[4] for box in pred_boxes])\n",
        "    results = {}\n",
        "    \n",
        "    for cls in classes:\n",
        "        gt_cls = [box for box in gt_boxes if box[4] == cls]\n",
        "        pred_cls = sorted([box for box in pred_boxes if box[4] == cls], \n",
        "                         key=lambda x: x[5], reverse=True)  # confidence로 정렬\n",
        "        \n",
        "        tp = np.zeros(len(pred_cls))\n",
        "        fp = np.zeros(len(pred_cls))\n",
        "        gt_matched = [False] * len(gt_cls)\n",
        "        \n",
        "        # 각 예측에 대해 매칭\n",
        "        for i, pred in enumerate(pred_cls):\n",
        "            best_iou = 0\n",
        "            best_gt_idx = -1\n",
        "            \n",
        "            for j, gt in enumerate(gt_cls):\n",
        "                if gt_matched[j]:\n",
        "                    continue\n",
        "                \n",
        "                iou = compute_iou(pred[:4], gt[:4])\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = j\n",
        "            \n",
        "            if best_iou >= iou_threshold:\n",
        "                tp[i] = 1\n",
        "                gt_matched[best_gt_idx] = True\n",
        "            else:\n",
        "                fp[i] = 1\n",
        "        \n",
        "        # 누적 TP, FP\n",
        "        tp_cumsum = np.cumsum(tp)\n",
        "        fp_cumsum = np.cumsum(fp)\n",
        "        \n",
        "        # Precision, Recall\n",
        "        recalls = tp_cumsum / len(gt_cls) if len(gt_cls) > 0 else np.zeros(len(pred_cls))\n",
        "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n",
        "        \n",
        "        # AP 계산\n",
        "        ap = calculate_ap(recalls, precisions)\n",
        "        results[cls] = {\n",
        "            'ap': ap,\n",
        "            'precision': precisions,\n",
        "            'recall': recalls\n",
        "        }\n",
        "    \n",
        "    # mAP 계산\n",
        "    map_score = np.mean([results[cls]['ap'] for cls in results])\n",
        "    results['mAP'] = map_score\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"평가 함수 정의 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: KITTI 형식 평가\n",
        "def evaluate_kitti_format(pred_dir, gt_dir, class_mapping=None):\n",
        "    \"\"\"\n",
        "    KITTI 형식의 예측 및 Ground Truth 평가\n",
        "    \"\"\"\n",
        "    if class_mapping is None:\n",
        "        class_mapping = {\n",
        "            'Car': 0, 'Van': 1, 'Truck': 2, 'Pedestrian': 3,\n",
        "            'Person_sitting': 4, 'Cyclist': 5, 'Tram': 6, 'Misc': 7\n",
        "        }\n",
        "    \n",
        "    pred_files = sorted(pred_dir.glob(\"*.txt\"))\n",
        "    gt_files = sorted(gt_dir.glob(\"*.txt\"))\n",
        "    \n",
        "    all_results = defaultdict(list)\n",
        "    \n",
        "    for pred_file, gt_file in zip(pred_files, gt_files):\n",
        "        # 예측 로드\n",
        "        pred_boxes = []\n",
        "        if pred_file.exists():\n",
        "            with open(pred_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) >= 6:\n",
        "                        class_name = parts[0]\n",
        "                        conf = float(parts[1])\n",
        "                        x1, y1, x2, y2 = map(float, parts[2:6])\n",
        "                        class_id = class_mapping.get(class_name, -1)\n",
        "                        if class_id >= 0:\n",
        "                            pred_boxes.append([x1, y1, x2, y2, class_id, conf])\n",
        "        \n",
        "        # Ground Truth 로드\n",
        "        gt_boxes = []\n",
        "        if gt_file.exists():\n",
        "            with open(gt_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) >= 15:\n",
        "                        class_name = parts[0]\n",
        "                        x1, y1, x2, y2 = map(float, parts[4:8])\n",
        "                        class_id = class_mapping.get(class_name, -1)\n",
        "                        if class_id >= 0:\n",
        "                            gt_boxes.append([x1, y1, x2, y2, class_id])\n",
        "        \n",
        "        # 평가\n",
        "        results = evaluate_detections(gt_boxes, pred_boxes)\n",
        "        for cls, result in results.items():\n",
        "            if cls != 'mAP':\n",
        "                all_results[cls].append(result['ap'])\n",
        "    \n",
        "    # 클래스별 평균 AP\n",
        "    class_aps = {}\n",
        "    for cls, aps in all_results.items():\n",
        "        class_aps[cls] = np.mean(aps)\n",
        "    \n",
        "    # 전체 mAP\n",
        "    map_score = np.mean(list(class_aps.values()))\n",
        "    \n",
        "    return {\n",
        "        'class_ap': class_aps,\n",
        "        'mAP': map_score\n",
        "    }\n",
        "\n",
        "print(\"KITTI 평가 함수 정의 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: 결과 리포트 생성\n",
        "def generate_evaluation_report(results, save_path=None):\n",
        "    \"\"\"\n",
        "    평가 결과 리포트 생성\n",
        "    \"\"\"\n",
        "    report = []\n",
        "    report.append(\"=\" * 60)\n",
        "    report.append(\"KITTI 평가 결과 리포트\")\n",
        "    report.append(\"=\" * 60)\n",
        "    report.append(\"\")\n",
        "    \n",
        "    if 'class_ap' in results:\n",
        "        report.append(\"클래스별 AP:\")\n",
        "        report.append(\"-\" * 60)\n",
        "        for cls, ap in sorted(results['class_ap'].items(), key=lambda x: x[1], reverse=True):\n",
        "            report.append(f\"  {cls:20s}: {ap:.4f}\")\n",
        "        report.append(\"\")\n",
        "    \n",
        "    if 'mAP' in results:\n",
        "        report.append(f\"mAP (mean Average Precision): {results['mAP']:.4f}\")\n",
        "        report.append(\"\")\n",
        "    \n",
        "    report_text = \"\\\\n\".join(report)\n",
        "    print(report_text)\n",
        "    \n",
        "    if save_path:\n",
        "        with open(save_path, 'w') as f:\n",
        "            f.write(report_text)\n",
        "        print(f\"\\\\n리포트 저장됨: {save_path}\")\n",
        "    \n",
        "    return report_text\n",
        "\n",
        "print(\"리포트 생성 함수 정의 완료\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
