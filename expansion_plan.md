# ììœ¨ì£¼í–‰ AI íŒŒì´í”„ë¼ì¸ í™•ì¥ ê³„íš

> YOLO, SAM, DINO, VQ-VAE ë“± ìµœì‹  ë¹„ì „ ëª¨ë¸ì„ í™œìš©í•œ ê¸°ëŠ¥ í™•ì¥ ë¡œë“œë§µ

## ê°œìš”

í˜„ì¬ íŒŒì´í”„ë¼ì¸(SimCLR + Behavioral Cloning + Safety Shield)ì„ ê¸°ë°˜ìœ¼ë¡œ, 
ìµœì‹  Foundation Modelë“¤ì„ í†µí•©í•˜ì—¬ ì¸ì‹ ëŠ¥ë ¥ê³¼ ì•ˆì „ì„±ì„ ê°•í™”í•©ë‹ˆë‹¤.

---

## 1. ê°ì²´ íƒì§€ (Object Detection)

### YOLO (You Only Look Once)

**ëª©ì **: ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ - ì°¨ëŸ‰, ë³´í–‰ì, ì‹ í˜¸ë“±, í‘œì§€íŒ ë“±

**í™œìš© ë°©ì•ˆ**:
```
ì´ë¯¸ì§€ â†’ YOLOv8 â†’ ë°”ìš´ë”© ë°•ìŠ¤ + í´ë˜ìŠ¤ + ì‹ ë¢°ë„
                         â†“
              Policy Head ì¶”ê°€ ì…ë ¥ìœ¼ë¡œ í™œìš©
```

**êµ¬í˜„ ê³„íš**:
- [ ] `08_yolo_detection.ipynb` - YOLOv8 í†µí•©
- [ ] íƒì§€ ê²°ê³¼ë¥¼ Policy ì…ë ¥ íŠ¹ì§•ìœ¼ë¡œ ì¶”ê°€
- [ ] íƒì§€ ê¸°ë°˜ Safety Shield ê°•í™”

---

## 2. ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ (Semantic Segmentation)

### SAM (Segment Anything Model)

**ëª©ì **: ë„ë¡œ, ì°¨ì„ , ë³´í–‰êµ¬ì—­ ë“± í”½ì…€ ë‹¨ìœ„ ë¶„í• 

**êµ¬í˜„ ê³„íš**:
- [ ] `09_sam_segmentation.ipynb` - SAM í†µí•©
- [ ] ì£¼í–‰ ê°€ëŠ¥ ì˜ì—­(Drivable Area) ì¶”ì¶œ
- [ ] ì°¨ì„  ê°ì§€ ë° ì°¨ì„  ì´íƒˆ ê²½ê³ 

---

## 3. ìê¸°ì§€ë„ ì‹œê° í‘œí˜„ í•™ìŠµ (Self-supervised Visual Representation)

### DINO / DINOv2

**ëª©ì **: ë ˆì´ë¸” ì—†ì´ ê°•ë ¥í•œ ì‹œê° íŠ¹ì§• í•™ìŠµ

**í™œìš© ë°©ì•ˆ**:
- **Encoder êµì²´**: ResNet-50 â†’ ViT + DINOv2 ê°€ì¤‘ì¹˜
- **Feature Quality í–¥ìƒ**: ë” í’ë¶€í•œ ì‹œë§¨í‹± ì •ë³´

**êµ¬í˜„ ê³„íš**:
- [ ] `10_dino_encoder.ipynb` - DINOv2 í†µí•©
- [ ] SimCLR vs DINOv2 ì„±ëŠ¥ ë¹„êµ

---

## 4. ê¹Šì´ ì¶”ì • (Depth Estimation)

### Depth Anything / MiDaS

**ëª©ì **: ë‹¨ì•ˆ ì¹´ë©”ë¼ë¡œ ê¹Šì´ ì •ë³´ ì¶”ì •

**êµ¬í˜„ ê³„íš**:
- [ ] `11_depth_estimation.ipynb` - ê¹Šì´ ì¶”ì • í†µí•©
- [ ] ê¹Šì´ ì •ë³´ ê¸°ë°˜ ì¥ì• ë¬¼ ê±°ë¦¬ ê³„ì‚°
- [ ] Bird's Eye View (BEV) ë³€í™˜

---

## 5. ë©€í‹°ëª¨ë‹¬ ì´í•´ (Multimodal Understanding)

### CLIP / BLIP-2

**ëª©ì **: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì´í•´ë¡œ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ë¥˜

**êµ¬í˜„ ê³„íš**:
- [ ] `12_clip_scene_understanding.ipynb` - CLIP í†µí•©
- [ ] ìì—°ì–´ ì¿¼ë¦¬ ê¸°ë°˜ ìƒí™© ë¶„ë¥˜

---

## 6. ë¹„ë””ì˜¤ ì´í•´ (Video Understanding)

### VideoMAE / TimeSformer

**ëª©ì **: ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ í™œìš©í•œ í–‰ë™ ì˜ˆì¸¡

**êµ¬í˜„ ê³„íš**:
- [ ] `13_video_understanding.ipynb` - ë¹„ë””ì˜¤ ëª¨ë¸ í†µí•©
- [ ] ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ Policy

---

## 7. World Model: VAE ê¸°ë°˜ ë¯¸ë˜ ì˜ˆì¸¡

### í•µì‹¬ ì•„ì´ë””ì–´

AutoEncoder ê¸°ë°˜ World Modelë¡œ **ë¯¸ë˜ ìƒíƒœë¥¼ Latent Spaceì—ì„œ ì˜ˆì¸¡**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    World Model Architecture                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   ê³¼ê±° ìƒíƒœ (t-k ~ t)          ë¯¸ë˜ ì˜ˆì¸¡ (t+1 ~ t+n)        â”‚
â”‚   â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”           â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”            â”‚
â”‚   â”‚I_1â”‚I_2â”‚...â”‚I_tâ”‚ â”€â”€â”€â”€â”€â”€â–¶   â”‚áº_1â”‚áº_2â”‚...â”‚áº_nâ”‚            â”‚
â”‚   â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜   (ì˜ˆì¸¡)  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜            â”‚
â”‚         â”‚                           â”‚                        â”‚
â”‚         â–¼                           â–¼                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚   Encoder   â”‚             â”‚   Decoder   â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚         â”‚                            â”‚                       â”‚
â”‚         â–¼                            â–¼                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚  Latent Z_t â”‚ â”€â”€â–¶ RNN â”€â”€â–¶ â”‚ Latent áº_t+1â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                      â”‚                       â”‚
â”‚                                      â–¼                       â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                         â”‚ Policy Head        â”‚              â”‚
â”‚                         â”‚ (Latent ê¸°ë°˜ ê²°ì •) â”‚              â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**êµ¬í˜„ ê³„íš**: `14_world_model.ipynb`

---

## 8. VQ-VAE: ì´ì‚° Latent í‘œí˜„ì„ í†µí•œ ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì¸¡
# ???????????? ë§Œì•½ z-score ì •ê·œí™”ë¥¼ í†µí•´ logitê°’ì— íŠ€ëŠ” í˜„ìƒì„ ìœ ë„í•œë‹¤ë©´ ????????Posterior Collapse ë¶•ê´´ì‹œ kl ê°’ê³¼ recon ê°’ì´ ë§¤ìš° ë†’ì•„ì¡Œë˜ ê¸°ì–µ, ì•ŒíŒŒê°’ ì¡°ì •ì´ ê²½í—˜ì— ê·¼ê±°í•´ì„œ ë•Œë ¤ë§ì¶”ëŠ” í˜•ì‹ì´ì—ˆë˜ê²ƒ ê°™ë‹¤.
### VQ-VAE vs VAE

| í•­ëª© | VAE | VQ-VAE |
|------|-----|--------|
| Latent í˜•íƒœ | ì—°ì† (Continuous) | ì´ì‚° (Discrete) |
| í‘œí˜„ | z âˆˆ â„^d | z âˆˆ {e_1, e_2, ..., e_K} |
| Posterior Collapse | ë°œìƒ ê°€ëŠ¥ | ê±°ì˜ ì—†ìŒ |
| ì¬êµ¬ì„± í’ˆì§ˆ | íë¦¿í•¨ | ì„ ëª…í•¨ |
| ìƒì„± ëª¨ë¸ ê²°í•© | ì–´ë ¤ì›€ | Autoregressive ëª¨ë¸ê³¼ ê²°í•© ìš©ì´ |

### VQ-VAE ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     VQ-VAE Architecture                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚   Input Image          Encoder Output         Quantized        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚         â”‚ â”€â”€â”€â”€â”€â”€â–¶  â”‚  z_e    â”‚ â”€â”€â”€â”€â”€â”€â–¶   â”‚  z_q    â”‚      â”‚
â”‚   â”‚  x      â”‚          â”‚ (ì—°ì†)   â”‚  (ì–‘ìí™”) â”‚ (ì´ì‚°)   â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                              â”‚                      â”‚          â”‚
â”‚                              â–¼                      â–¼          â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚                        â”‚      Codebook E             â”‚       â”‚
â”‚                        â”‚  {e_1, e_2, ..., e_K}       â”‚       â”‚
â”‚                        â”‚  K = 512 ~ 8192 ë²¡í„°        â”‚       â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                â”‚
â”‚   z_q = argmin ||z_e - e_k||  (ê°€ì¥ ê°€ê¹Œìš´ ì½”ë“œë¶ ë²¡í„°)       â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### VQ-VAE ì†ì‹¤ í•¨ìˆ˜

```python
# VQ-VAE Loss = Reconstruction + Codebook + Commitment
loss = (
    # 1. ì¬êµ¬ì„± ì†ì‹¤
    MSE(decoder(z_q), x) +
    
    # 2. Codebook ì†ì‹¤ (ì½”ë“œë¶ ë²¡í„°ë¥¼ encoder ì¶œë ¥ìœ¼ë¡œ)
    MSE(z_e.detach(), z_q) +
    
    # 3. Commitment ì†ì‹¤ (encoder ì¶œë ¥ì„ ì½”ë“œë¶ ë²¡í„°ë¡œ)
    Î² * MSE(z_e, z_q.detach())
)

# Straight-Through Estimatorë¡œ ì—­ì „íŒŒ
z_q = z_e + (z_q - z_e).detach()  # ìˆœì „íŒŒ: z_q, ì—­ì „íŒŒ: z_e
```

### VQ-VAE-2 (Hierarchical)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              VQ-VAE-2 Hierarchical                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚   Level 1 (Top): ì „ì—­ êµ¬ì¡° (32x32 latent)          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚  ë„ë¡œ ë ˆì´ì•„ì›ƒ, ì „ì²´ ì¥ë©´ êµ¬ì¡°       â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                     â†“                               â”‚
â”‚   Level 2 (Bottom): ì„¸ë¶€ ì •ë³´ (64x64 latent)       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚  ì°¨ëŸ‰ ë””í…Œì¼, ë³´í–‰ì, ì‹ í˜¸ë“± ìƒíƒœ    â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                     â”‚
â”‚   ì¥ì : ë‹¤ì–‘í•œ ì¶”ìƒí™” ìˆ˜ì¤€ì˜ í‘œí˜„ í•™ìŠµ             â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ììœ¨ì£¼í–‰ì—ì„œì˜ VQ-VAE í™œìš©

**1. ì‹œë‚˜ë¦¬ì˜¤ í´ëŸ¬ìŠ¤í„°ë§**
```python
# ì´ì‚° ì½”ë“œë¡œ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ë¥˜
scenario_codes = vq_encoder(driving_sequence)
# codes: [K, K, K, ...] ì‹œí€€ìŠ¤

# ìœ ì‚¬ ì‹œë‚˜ë¦¬ì˜¤ ê²€ìƒ‰
similar_scenarios = find_by_code_pattern([23, 45, 12, ...])
```

**2. Autoregressive ë¯¸ë˜ ì˜ˆì¸¡**
```python
# ì´ì‚° ì½”ë“œ ì‹œí€€ìŠ¤ â†’ GPT ìŠ¤íƒ€ì¼ ì˜ˆì¸¡
past_codes = vq_encode(past_frames)  # [z_1, z_2, ..., z_t]
future_codes = transformer.generate(past_codes, n_future=10)
future_frames = vq_decode(future_codes)
```

**3. ìœ„í—˜ ì‹œë‚˜ë¦¬ì˜¤ íƒì§€**
```python
# ì½”ë“œë¶ì—ì„œ ìœ„í—˜ ì½”ë“œ íŒ¨í„´ ì •ì˜
danger_codes = {
    'sudden_brake': [102, 203, 405],
    'pedestrian_cross': [55, 89, 123],
    'red_light_run': [301, 402, 501]
}

if current_code_pattern matches danger_codes:
    activate_safety_shield()
```

**êµ¬í˜„ ê³„íš**: `15_vqvae_world_model.ipynb`

---

## 9. êµ¬í˜„ ìš°ì„ ìˆœìœ„

| ìˆœìœ„ | ê¸°ëŠ¥ | ëª¨ë¸ | ë‚œì´ë„ | íš¨ê³¼ |
|------|------|------|--------|------|
| 1 | ê°ì²´ íƒì§€ | YOLOv8 | â­â­ | â­â­â­â­â­ |
| 2 | ì‹œê° í‘œí˜„ | DINOv2 | â­â­ | â­â­â­â­ |
| 3 | ê¹Šì´ ì¶”ì • | Depth Anything | â­â­ | â­â­â­â­ |
| 4 | ì„¸ê·¸ë©˜í…Œì´ì…˜ | SAM | â­â­â­ | â­â­â­â­ |
| 5 | World Model | VQ-VAE | â­â­â­â­ | â­â­â­â­â­ |
| 6 | ìƒí™© ì´í•´ | CLIP | â­â­â­ | â­â­â­ |

---

## 10. ì˜ˆìƒ ë…¸íŠ¸ë¶ ëª©ë¡

```
notebook/
â”œâ”€â”€ 01_carla_setup.ipynb           âœ… ì™„ë£Œ
â”œâ”€â”€ 02_data_collection.ipynb       âœ… ì™„ë£Œ
â”œâ”€â”€ 03_kitti_exploration.ipynb     âœ… ì™„ë£Œ
â”œâ”€â”€ 04_ssl_pretraining.ipynb       âœ… ì™„ë£Œ
â”œâ”€â”€ 05_bc_training.ipynb           âœ… ì™„ë£Œ
â”œâ”€â”€ 06_safety_shield.ipynb         âœ… ì™„ë£Œ
â”œâ”€â”€ 07_evaluation.ipynb            âœ… ì™„ë£Œ
â”‚
â”œâ”€â”€ 08_yolo_detection.ipynb        ğŸ”œ Phase 1
â”œâ”€â”€ 09_sam_segmentation.ipynb      ğŸ”œ Phase 1
â”œâ”€â”€ 10_dino_encoder.ipynb          ğŸ”œ Phase 1
â”œâ”€â”€ 11_depth_estimation.ipynb      ğŸ”œ Phase 2
â”œâ”€â”€ 12_clip_scene_understanding.ipynb  ğŸ”œ Phase 2
â”œâ”€â”€ 13_video_understanding.ipynb   ğŸ”œ Phase 2
â”œâ”€â”€ 14_world_model.ipynb           ğŸ”œ Phase 3 (VAE)
â””â”€â”€ 15_vqvae_world_model.ipynb     ğŸ”œ Phase 3 (VQ-VAE)
```

---

## 11. ì¢…í•© ì°¸ê³  ë…¼ë¬¸ ëª©ë¡

### ê°ì²´ íƒì§€ (Object Detection)

| ë…¼ë¬¸ | ì—°ë„ | í•µì‹¬ ê¸°ì—¬ |
|------|------|-----------|
| **YOLOv8** (Ultralytics) | 2023 | ì‹¤ì‹œê°„ íƒì§€+ì„¸ê·¸ë©˜í…Œì´ì…˜ í†µí•© |
| **YOLOv9** (Wang et al.) | 2024 | PGI + GELAN ì•„í‚¤í…ì²˜ |
| **YOLOv10** (THU) | 2024 | NMS-free, ë” ë¹ ë¥¸ ì¶”ë¡  |
| **RT-DETR** (Baidu) | 2023 | Real-time DETR, Transformer ê¸°ë°˜ |
| **Co-DETR** (Yu et al.) | 2023 | COCO SOTA, í˜‘ì—… í•™ìŠµ |
| **DINO Detection** (Zhang et al.) | 2023 | DETR ê°œì„ , ë” ë¹ ë¥¸ ìˆ˜ë ´ |

### ì„¸ê·¸ë©˜í…Œì´ì…˜ (Segmentation)

| ë…¼ë¬¸ | ì—°ë„ | í•µì‹¬ ê¸°ì—¬ |
|------|------|-----------|
| **SAM** (Kirillov et al.) | 2023 | Promptable ì„¸ê·¸ë©˜í…Œì´ì…˜, 11M ì´ë¯¸ì§€ |
| **SAM 2** (Ravi et al.) | 2024 | ë¹„ë””ì˜¤ ì„¸ê·¸ë©˜í…Œì´ì…˜, ì‹œê°„ ì¼ê´€ì„± |
| **FastSAM** (Zhao et al.) | 2023 | 50ë°° ë¹ ë¥¸ SAM, YOLO ê¸°ë°˜ |
| **EfficientSAM** (Xiong et al.) | 2024 | SAM ê²½ëŸ‰í™”, ëª¨ë°”ì¼ ì§€ì› |
| **Grounded SAM** (Liu et al.) | 2023 | í…ìŠ¤íŠ¸â†’ì„¸ê·¸ë©˜í…Œì´ì…˜ |

### ìê¸°ì§€ë„ í•™ìŠµ (Self-supervised Learning)

| ë…¼ë¬¸ | ì—°ë„ | í•µì‹¬ ê¸°ì—¬ |
|------|------|-----------|
| **SimCLR** (Chen et al.) | 2020 | Contrastive learning, ê°•ë ¥í•œ augmentation |
| **MoCo v3** (Chen et al.) | 2021 | ViT + Momentum Contrast |
| **DINO** (Caron et al.) | 2021 | Self-distillation, ViT |
| **DINOv2** (Oquab et al.) | 2023 | 142M ì´ë¯¸ì§€, ë²”ìš© íŠ¹ì§• |
| **I-JEPA** (Assran et al.) | 2023 | ì´ë¯¸ì§€ ì˜ˆì¸¡ ê¸°ë°˜ í‘œí˜„ í•™ìŠµ |
| **MAE** (He et al.) | 2022 | Masked Autoencoder, íš¨ìœ¨ì  í•™ìŠµ |

### ê¹Šì´ ì¶”ì • (Depth Estimation)

| ë…¼ë¬¸ | ì—°ë„ | í•µì‹¬ ê¸°ì—¬ |
|------|------|-----------|
| **Depth Anything** (Yang et al.) | 2024 | 62M ì´ë¯¸ì§€, ë²”ìš© ê¹Šì´ |
| **Depth Anything V2** (Yang et al.) | 2024 | ë” ì •í™•í•œ ê¹Šì´ |
| **MiDaS v3.1** (Ranftl et al.) | 2022 | Zero-shot ê¹Šì´ |
| **ZoeDepth** (Bhat et al.) | 2023 | Metric depth, ì‹¤ì œ ê±°ë¦¬ ì¶”ì • |
| **UniDepth** (Piccinelli et al.) | 2024 | Camera-agnostic ê¹Šì´ |
| **Metric3D** (Yin et al.) | 2023 | ì¹´ë©”ë¼ ë‚´ì¬ íŒŒë¼ë¯¸í„° ë¶ˆí•„ìš” |

### ë©€í‹°ëª¨ë‹¬ (Vision-Language)

| ë…¼ë¬¸ | ì—°ë„ | í•µì‹¬ ê¸°ì—¬ |
|------|------|-----------|
| **CLIP** (Radford et al.) | 2021 | í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ëŒ€ì¡° í•™ìŠµ |
| **BLIP-2** (Li et al.) | 2023 | Frozen LLM + Vision |
| **LLaVA 1.5** (Liu et al.) | 2024 | Visual Instruction Tuning |
| **GPT-4V** (OpenAI) | 2023 | ë©€í‹°ëª¨ë‹¬ GPT-4 |
| **Gemini** (Google) | 2024 | ë„¤ì´í‹°ë¸Œ ë©€í‹°ëª¨ë‹¬ |
| **InternVL 2** (Chen et al.) | 2024 | ì˜¤í”ˆì†ŒìŠ¤ GPT-4V ëŒ€ì•ˆ |
| **Qwen-VL** (Bai et al.) | 2023 | í•œì¤‘ì˜ ë©€í‹°ëª¨ë‹¬ |

### ë¹„ë””ì˜¤ ì´í•´ (Video Understanding)

| ë…¼ë¬¸ | ì—°ë„ | í•µì‹¬ ê¸°ì—¬ |
|------|------|-----------|
| **VideoMAE** (Tong et al.) | 2022 | ë¹„ë””ì˜¤ MAE, ì‹œê³µê°„ í•™ìŠµ |
| **VideoMAE V2** (Wang et al.) | 2023 | Dual masking, ë” ë‚˜ì€ í‘œí˜„ |
| **TimeSformer** (Bertasius et al.) | 2021 | Divided space-time attention |
| **Video-LLaVA** (Lin et al.) | 2023 | ë¹„ë””ì˜¤ ì´í•´ LLM |
| **InternVideo2** (Wang et al.) | 2024 | ëŒ€ê·œëª¨ ë¹„ë””ì˜¤ ëª¨ë¸ |

### World Model / ìƒì„± ëª¨ë¸

| ë…¼ë¬¸ | ì—°ë„ | í•µì‹¬ ê¸°ì—¬ |
|------|------|-----------|
| **World Models** (Ha & Schmidhuber) | 2018 | VAE+RNN í™˜ê²½ ëª¨ë¸ë§ |
| **Dreamer V1** (Hafner et al.) | 2020 | RSSM, Latent Imagination |
| **Dreamer V2** (Hafner et al.) | 2021 | Discrete Latent, KL ë°¸ëŸ°ì‹± |
| **Dreamer V3** (Hafner et al.) | 2023 | ë²”ìš© World Model, 150+ íƒœìŠ¤í¬ |
| **VQ-VAE** (van den Oord et al.) | 2017 | ì´ì‚° Latent, ì½”ë“œë¶ |
| **VQ-VAE-2** (Razavi et al.) | 2019 | Hierarchical VQ-VAE |
| **VQ-GAN** (Esser et al.) | 2021 | VQ-VAE + GAN, ì„ ëª…í•œ ì¬êµ¬ì„± |
| **MAGVIT-2** (Yu et al.) | 2024 | ë¹„ë””ì˜¤ í† í¬ë‚˜ì´ì € SOTA |

### ììœ¨ì£¼í–‰ íŠ¹í™” (Autonomous Driving)

| ë…¼ë¬¸ | ì—°ë„ | í•µì‹¬ ê¸°ì—¬ |
|------|------|-----------|
| **MILE** (Hu et al.) | 2022 | ììœ¨ì£¼í–‰ World Model |
| **GAIA-1** (Wayve) | 2023 | 9B World Model, ë¹„ë””ì˜¤ ìƒì„± |
| **UniAD** (Hu et al.) | 2023 | í†µí•© ììœ¨ì£¼í–‰, E2E |
| **VAD** (Jiang et al.) | 2023 | Vectorized Scene í‘œí˜„ |
| **DriveVLM** (Sima et al.) | 2024 | VLM ììœ¨ì£¼í–‰ í†µí•© |
| **GenAD** (Yang et al.) | 2024 | ìƒì„± E2E ììœ¨ì£¼í–‰ |
| **OccWorld** (Zheng et al.) | 2024 | 4D Occupancy World Model |
| **Drive-WM** (Wang et al.) | 2024 | í™•ì‚° ê¸°ë°˜ World Model |
| **BEVFormer** (Li et al.) | 2022 | BEV ë³€í™˜ ê¸°ë°˜ ì¸ì‹ |
| **StreamPETR** (Wang et al.) | 2023 | ìŠ¤íŠ¸ë¦¬ë° 3D ê°ì²´ íƒì§€ |

### Imitation Learning / Behavioral Cloning

| ë…¼ë¬¸ | ì—°ë„ | í•µì‹¬ ê¸°ì—¬ |
|------|------|-----------|
| **DAgger** (Ross et al.) | 2011 | Distribution Shift í•´ê²° |
| **GAIL** (Ho & Ermon) | 2016 | GAN ê¸°ë°˜ Imitation |
| **Diffusion Policy** (Chi et al.) | 2023 | í™•ì‚° ëª¨ë¸ ê¸°ë°˜ Policy |
| **ACT** (Zhao et al.) | 2023 | Action Chunking Transformer |
| **RT-1/RT-2** (Brohan et al.) | 2023 | ë¡œë´‡ Transformer |

### ì•ˆì „ ë° ê°•í™”í•™ìŠµ (Safe RL)

| ë…¼ë¬¸ | ì—°ë„ | í•µì‹¬ ê¸°ì—¬ |
|------|------|-----------|
| **Safe RL Survey** (GarcÃ­a & FernÃ¡ndez) | 2015 | ì•ˆì „ RL ì¢…í•© ì„œë² ì´ |
| **CPO** (Achiam et al.) | 2017 | Constrained Policy Optimization |
| **Safety Gym** (Ray et al.) | 2019 | ì•ˆì „ RL ë²¤ì¹˜ë§ˆí¬ |
| **Shield** (Alshiekh et al.) | 2018 | í˜•ì‹ ê²€ì¦ ê¸°ë°˜ Safety |

---

## 12. ìµœì‹  íŠ¸ë Œë“œ (2024)

### 1. Foundation Model + ììœ¨ì£¼í–‰
- VLM(Vision-Language Model)ì„ ììœ¨ì£¼í–‰ì— ì ìš©
- GPT-4V/Gemini ìŠ¤íƒ€ì¼ ëª¨ë¸ë¡œ ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ ì´í•´

### 2. World Model ê¸°ë°˜ E2E
- GAIA-1, OccWorld ìŠ¤íƒ€ì¼ 4D ì˜ˆì¸¡
- í™•ì‚° ëª¨ë¸ ê¸°ë°˜ ë¯¸ë˜ ìƒì„±

### 3. Occupancy Networks
- 3D Occupancy Gridë¡œ ì¥ë©´ í‘œí˜„
- LiDAR ì—†ì´ ì¹´ë©”ë¼ë§Œìœ¼ë¡œ 3D ì´í•´

### 4. ì‹ ê²½ë§ ê²½ëŸ‰í™”
- ëª¨ë°”ì¼/ì„ë² ë””ë“œ í™˜ê²½ ë°°í¬
- ì–‘ìí™”, í”„ë£¨ë‹, ì§€ì‹ ì¦ë¥˜

---

*ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2024-12-14*
